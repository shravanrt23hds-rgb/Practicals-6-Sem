PRACTICAL 1 - Introduction to Business Analytics: 
a. Collect data from a real-life business scenario and perform exploratory data 
analysis (EDA) to gain insights into the dataset. 

import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
# Load the dataset 
df = pd.read_excel("Online Retail.xlsx")
df.head()
# Data preprocessing 
df = df.dropna() # Drop rows with missing values 
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']) # Convert InvoiceDate to datetime format 
df.head()
# Exploratory Data Analysis (EDA) 
# Summary statistics 
print(df.describe()) 
# Visualize the distribution of Quantity sold 
plt.figure(figsize=(10, 6)) 
sns.histplot(df['Quantity'], bins=50, kde=True) 
plt.title('Distribution of Quantity Sold') 
plt.xlabel('Quantity') 
plt.ylabel('Frequency') 
plt.show() 
# Time series analysis 
df['InvoiceYearMonth'] = df['InvoiceDate'].dt.to_period('M') 
monthly_sales = df.groupby('InvoiceYearMonth')['Quantity'].sum()
plt.figure(figsize=(10, 6)) 
monthly_sales.plot(marker='o') 
plt.title('Monthly Sales Trend') 
plt.xlabel('Month') 
plt.ylabel('Total Quantity Sold') 
plt.xticks(rotation=45) 
plt.grid(True) 
plt.show() 
# Customer analysis 
customer_orders = df.groupby('CustomerID')['InvoiceNo'].nunique() 
plt.figure(figsize=(10, 6)) 
sns.histplot(customer_orders, bins=30, kde=True) 
plt.title('Distribution of Number of Orders per Customer') 
plt.xlabel('Number of Orders') 
plt.ylabel('Number of Customers') 
plt.show() 
# Calculate customer lifetime value (CLV) 
df['TotalPrice'] = df['Quantity'] * df['UnitPrice'] 
customer_clv = df.groupby('CustomerID')['TotalPrice'].sum() 
df.head()
# Visualize customer CLV distribution 
plt.figure(figsize=(10, 6)) 
sns.histplot(customer_clv, bins=30, kde=True) 
plt.title('Distribution of Customer Lifetime Value') 
plt.xlabel('Customer Lifetime Value') 
plt.ylabel('Number of Customers') 
plt.show

b. Analyze customer data to identify trends and patterns that can be used for 
business decision-making. 

import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
# Load the dataset 
df = pd.read_excel("Online Retail.xlsx")
df.head()
# Data preprocessing 
df = df.dropna() # Drop rows with missing values 
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']) # Convert InvoiceDate to datetime format 
df.head()
# Customer analysis
customer_orders = df.groupby('CustomerID')['InvoiceNo'].nunique() 
plt.figure(figsize=(10, 6)) 
sns.histplot(customer_orders, bins=30, kde=True) 
plt.title('Distribution of Number of Orders per Customer') 
plt.xlabel('Number of Orders') 
plt.ylabel('Number of Customers') 
plt.show() 
# Calculate customer lifetime value (CLV) 
df['TotalPrice'] = df['Quantity'] * df['UnitPrice'] 
customer_clv = df.groupby('CustomerID')['TotalPrice'].sum() 
df.head()
# Visualize customer CLV distribution 
plt.figure(figsize=(10, 6)) 
sns.histplot(customer_clv, bins=30, kde=True) 
plt.title('Distribution of Customer Lifetime Value') 
plt.xlabel('Customer Lifetime Value') 
plt.ylabel('Number of Customers') 
plt.show() 

============================================================================================================================

PRACTICAL 2 - Describing the Distribution of a Variable: 
a. Obtain a dataset and calculate descriptive statistics (mean, median, mode, 
variance, etc.) for a specific variable of interest. 

import pandas as pd 
# Load the dataset 
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 
'hours-per-week', 'native-country', 'income'] 
df = pd.read_csv("adult.data", header=None, names=column_names, na_values=' ?')
# Display the first few rows of the dataset 
print("First few rows of the dataset:") 
print(df.head()) 
# Calculate descriptive statistics for the 'age' variable 
age_stats = df['age'].describe() 
print("\nDescriptive statistics for 'age':") 
print(age_stats) 
# Calculate mode for 'age' 
age_mode = df['age'].mode() 
print("\nMode for 'age':") 
print(age_mode) 
# Calculate variance for 'age' 
age_variance = df['age'].var() 
print("\nVariance for 'age':", age_variance) 
# Visualize the distribution of 'age' with a histogram 
import matplotlib.pyplot as plt 
plt.hist(df['age'], bins=20, color='skyblue', edgecolor='black') 
plt.title('Distribution of Age') 
plt.xlabel('Age') 
plt.ylabel('Frequency') 
plt.show() 

b. Create visualizations (histograms, box plots) to depict the distribution of a 
variable and analyze its characteristics.

import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
# Load the dataset 
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 
'hours-per-week', 'native-country', 'income'] 
df = pd.read_csv("adult.data", header=None, names=column_names, na_values=' ?')
# Display the first few rows of the dataset 
print("First few rows of the dataset:") 
print(df.head()) 
# Plot histogram to visualize the distribution of 'age' 
plt.figure(figsize=(10, 6)) 
plt.hist(df['age'], bins=20, color='skyblue', edgecolor='black') 
plt.title('Distribution of Age') 
plt.xlabel('Age') 
plt.ylabel('Frequency') 
plt.grid(True) 
plt.show() 
# Plot box plot to visualize the distribution of 'age' 
plt.figure(figsize=(8, 6)) 
sns.boxplot(x=df['age'], color='lightblue') 
plt.title('Box Plot of Age') 
plt.xlabel('Age') 
plt.show() 

=============================================================================================================================

PRACTICAL 3 - Finding Relationships Among Variables: 
a. Use a dataset with multiple variables and perform correlation analysis to 
determine the strength and direction of relationships between pairs of variables. 

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression 
# Generate synthetic dataset 
np.random.seed(0) 
advertising_expenditure = np.random.normal(50, 20, 100) 
sales_revenue = 100 + 3 * advertising_expenditure + np.random.normal(0, 10, 100) 
data = pd.DataFrame({'Advertising Expenditure': advertising_expenditure, 'Sales Revenue': sales_revenue}) 
# Visualize the synthetic dataset 
plt.figure(figsize=(10, 6)) 
plt.scatter(data['Advertising Expenditure'], data['Sales Revenue'], color='blue') 
plt.title('Relationship between Advertising Expenditure and Sales Revenue') 
plt.xlabel('Advertising Expenditure') 
plt.ylabel('Sales Revenue') 
plt.grid(True) 
plt.show() 
# Perform linear regression 
X = data[['Advertising Expenditure']] 
y = data['Sales Revenue'] 
model = LinearRegression() 
model.fit(X, y) 
# Visualize the regression line 
plt.figure(figsize=(10, 6)) 
plt.scatter(data['Advertising Expenditure'], data['Sales Revenue'], color='blue') 
plt.plot(data['Advertising Expenditure'], model.predict(X), color='red', linewidth=2) 
plt.title('Linear Regression: Advertising Expenditure vs. Sales Revenue') 
plt.xlabel('Advertising Expenditure') 
plt.ylabel('Sales Revenue') 
plt.grid(True) 
plt.show() 
# Print the coefficients 
print('Intercept:', model.intercept_) 
print('Coefficient:', model.coef_[0]) 

b. Perform multiple regression on the same dataset.Find R^2 score. Predict 
future sales based on new ad expenditure. 

import numpy as np  
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression 
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
# GENERATE SYNTHETIC DATASET
np.random.seed(0)
advertising_expenditure = np.random.normal(50, 20, 100)          # Feature 1
additional_expenditure = np.random.normal(30, 10, 100)           # Feature 2 (new)
# Sales depends on both expenditures now
sales_revenue = (100
                 + 3 * advertising_expenditure
                 + 1.5 * additional_expenditure
                 + np.random.normal(0, 10, 100))
data = pd.DataFrame({
    'Advertising Expenditure': advertising_expenditure,
    'Additional Expenditure': additional_expenditure,
    'Sales Revenue': sales_revenue
})
# Multiple Linear Regression
X = data[['Advertising Expenditure', 'Additional Expenditure']]
y = data['Sales Revenue']
# Split to test model accuracy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
model = LinearRegression()
model.fit(X_train, y_train)
# R² Score (Accuracy)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)
print("R² Score:", r2)
new_data = pd.DataFrame({
    'Advertising Expenditure': [60],
    'Additional Expenditure': [35]
})
future_sales_prediction = model.predict(new_data)
print("\nPredicted Future Sales for new data:", future_sales_prediction[0])
# 1. Visualize the Synthetic Dataset (3D Plot)
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(
    data['Advertising Expenditure'],
    data['Additional Expenditure'],
    data['Sales Revenue'],
    color='blue'
)
ax.set_title('3D View: Advertising vs Additional Expenditure vs Sales Revenue')
ax.set_xlabel('Advertising Expenditure')
ax.set_ylabel('Additional Expenditure')
ax.set_zlabel('Sales Revenue')
plt.show()
# 2. Visualize the Regression Plane (3D)
# Create grid for plane
ad_range = np.linspace(data['Advertising Expenditure'].min(),
                       data['Advertising Expenditure'].max(), 20)

extra_range = np.linspace(data['Additional Expenditure'].min(),
                          data['Additional Expenditure'].max(), 20)
ad_grid, extra_grid = np.meshgrid(ad_range, extra_range)
# Predict sales values for the grid
plane_points = model.predict(
    np.column_stack((ad_grid.ravel(), extra_grid.ravel()))
)
plane_points = plane_points.reshape(ad_grid.shape)
# Plot the plane
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(
    data['Advertising Expenditure'],
    data['Additional Expenditure'],
    data['Sales Revenue'],
    color='blue'
)
ax.plot_surface(ad_grid, extra_grid, plane_points, alpha=0.5, color='red')
ax.set_title('Regression Plane for Multiple Linear Regression')
ax.set_xlabel('Advertising Expenditure')
ax.set_ylabel('Additional Expenditure')
ax.set_zlabel('Predicted Sales Revenue')
plt.show()

============================================================================================================================

PRACTICAL 4 - Probability and Probability Distributions: 
a. Simulate a probability experiment (e.g., rolling dice) using programming and 
calculate the probabilities of different outcomes. 

import random
def roll_dice(num_rolls):
    outcomes = [0] * 6 # Initialize outcomes counter for 6 sides of a die 
    for _ in range(num_rolls):
        outcome = random.randint(1, 6) # Simulate rolling a six-sided die 
        outcomes[outcome - 1] += 1 # Increment the count for the corresponding outcome
    probabilities = [count / num_rolls for count in outcomes] # Calculate probabilities 
    return probabilities 
num_rolls = 10000 
probabilities = roll_dice(num_rolls)
for outcome, probability in enumerate(probabilities, start=1):
    print(f"Probability of getting {outcome} is {probability:.4f}") 

b. Generate random numbers from various probability distributions (normal, 
uniform, exponential) and analyze their properties. 

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
# Generate random numbers from normal distribution
mean = 0
std_dev = 1
num_samples = 1000
normal_samples = np.random.normal(mean, std_dev, num_samples)
# Plot histogram 
plt.hist(normal_samples, bins=30, density=True, alpha=0.6, color='g') 
plt.title('Normal Distribution') 
plt.xlabel('Value') 
plt.ylabel('Frequency') 
plt.show() 
# Generate random numbers from uniform distribution
low = 0
high = 1
uniform_samples = np.random.uniform(low, high, num_samples)
# Plot histogram 
plt.hist(uniform_samples, bins=30, density=True, alpha=0.6, color='b')
plt.title('Uniform Distribution') 
plt.xlabel('Value') 
plt.ylabel('Frequency') 
plt.show() 
# Generate random numbers from exponential distribution 
lambda_param = 1 # Rate parameter 
exponential_samples = np.random.exponential(scale=1/lambda_param, size=num_samples) 
# Plot histogram 
plt.hist(exponential_samples, bins=30, density=True, alpha=0.6, color='r')
plt.title('Exponential Distribution') 
plt.xlabel('Value') 
plt.ylabel('Frequency') 
plt.show() 
# Lognormal Distribution
X = np.linspace(0, 6, 600)
std = 1
mean = 0
lognorm_distribution = stats.lognorm([std], loc=mean)
lognorm_distribution_pdf = lognorm_distribution.pdf(X)
fig, ax = plt.subplots(figsize=(9, 6))
plt.plot(X, lognorm_distribution_pdf, label="μ=0, σ=1")
ax.set_xticks(np.arange(min(X), max(X)))
std = 0.5
mean = 0
lognorm_distribution = stats.lognorm([std], loc=mean)
lognorm_distribution_pdf = lognorm_distribution.pdf(X)
plt.plot(X, lognorm_distribution_pdf, label="μ=0, σ=0.5")
std = 1.5
mean = 0
lognorm_distribution = stats.lognorm([std], loc=mean)
lognorm_distribution_pdf = lognorm_distribution.pdf(X)
plt.plot(X, lognorm_distribution_pdf, label="μ=1, σ=1.5")
plt.title("Lognormal Distribution")
plt.legend()
plt.show()
from scipy import stats
print(stats.poisson.pmf(k=9, mu=3))
# Poisson Distribution
X = stats.poisson.rvs(mu=3, size=600)
plt.subplots(figsize=(9, 6))
plt.hist(X, density=True, edgecolor="black")
plt.title("Poisson Distribution")
plt.show()
mu = 0
variance = 1
sigma = np.sqrt(variance)
X = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
plt.subplots(figsize=(9, 6))
plt.plot(X, stats.norm.pdf(X, mu, sigma))
plt.title("Normal Distribution")
plt.show()
a = 0
b = 50
size = 5000
X_continuous = np.linspace(a, b, size)
continuous_uniform = stats.uniform(loc=a, scale=b)
continuous_uniform_pdf = continuous_uniform.pdf(X_continuous)
X_discrete = np.arange(1, 7)
discrete_uniform = stats.randint(1, 7)
discrete_uniform_pmf = discrete_uniform.pmf(X_discrete)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))
ax[0].bar(X_discrete, discrete_uniform_pmf)
ax[0].set_xlabel("x")
ax[0].set_ylabel("probability")
ax[0].set_title("Discrete Uniform Distribution")
ax[1].plot(X_continuous, continuous_uniform_pdf)
ax[1].set_xlabel("x")
ax[1].set_ylabel("Probability")
ax[1].set_title("Continuous Uniform Distribution")
plt.show()
X = np.random.binomial(n=1, p=0.5, size=2000)
plt.subplots(figsize=(9, 6))
plt.hist(X)
plt.title("Binomial Distribution")
plt.show()
X = np.arange(0, 6, 0.25)
plt.subplots(figsize=(9, 6))
plt.plot(X, stats.chi2.pdf(X, df=1), label="1 d.o.f")
plt.plot(X, stats.chi2.pdf(X, df=2), label="2 d.o.f")
plt.plot(X, stats.chi2.pdf(X, df=3), label="3 d.o.f")
plt.title("Chi-squared Distribution")
plt.legend()
plt.show()
X = np.linspace(0, 5, 6000)
exponential_distribution = stats.expon.pdf(X, loc=0, scale=1)
plt.subplots(figsize=(9, 6))
plt.plot(X, exponential_distribution)
plt.title("Exponential Distribution")
plt.show()

==============================================================================================================================

PRACTICAL 5 - Decision Making under Uncertainty: 
a. Develop a decision tree model to make business decisions considering 
uncertainties and associated probabilities at each decision point. 
b. Apply the concept of expected value to evaluate different decision alternatives 
and select the optimal one. 

import matplotlib.pyplot as plt
import matplotlib.patches as patches
decision_tree = {
    'Decision 1': {'Option A': (0.4, 100), 'Option B': (0.6, 80)},
    'Decision 2': {'Option C': (0.3, 50), 'Option D': (0.7, 70)}
}


def visualize_decision_tree(tree):

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 6)
    ax.axis('off')

    # Find best option
    best = max(
        ((d, o, p * pf)
         for d, opts in tree.items()
         for o, (p, pf) in opts.items()),
        key=lambda x: x[2]
    )

    # Draw starting point
    ax.add_patch(
        patches.Circle((1, 3), 0.3, color='lightblue',
                       ec='black', linewidth=2)
    )
    ax.text(1, 3, 'START',
            ha='center', va='center',
            fontsize=9, weight='bold')

    # Draw tree
    y_pos = [4.5, 1.5]

    for i, (decision, options) in enumerate(tree.items()):

        is_best_dec = decision == best[0]

        # Line from start to decision
        ax.plot(
            [1.3, 2.5],
            [3, y_pos[i]],
            'g-' if is_best_dec else 'k-',
            linewidth=2
        )

        # Decision node
        ax.add_patch(
            patches.Rectangle(
                (2.5, y_pos[i] - 0.3),
                0.6,
                0.6,
                color='gold' if is_best_dec else 'lightgreen',
                ec='black',
                linewidth=2
            )
        )
        ax.text(2.8, y_pos[i], decision,
                ha='center', va='center',
                fontsize=9, weight='bold')

        if is_best_dec:
            ax.text(2.2, y_pos[i] + 0.4,
                    '★', fontsize=20, color='darkgreen')

        # Options
        for j, (opt, (prob, payoff)) in enumerate(options.items()):

            opt_y = y_pos[i] + (0.8 if j == 0 else -0.8)
            is_best = decision == best[0] and opt == best[1]

            color = 'gold' if is_best else 'lightcoral'
            lw = 3 if is_best else 1

            ax.plot(
                [3.1, 5.5],
                [y_pos[i], opt_y],
                'g-' if is_best else 'k-',
                linewidth=lw
            )

            ax.text(
                4.3,
                (y_pos[i] + opt_y) / 2 + 0.1,
                f'P={prob}',
                fontsize=7,
                bbox=dict(
                    boxstyle='round',
                    fc='lime' if is_best else 'yellow',
                    alpha=0.7
                )
            )

            ax.add_patch(
                patches.Circle(
                    (5.8, opt_y),
                    0.25,
                    color=color,
                    ec='black',
                    linewidth=lw
                )
            )
            ax.text(5.8, opt_y, opt,
                    ha='center', va='center',
                    fontsize=8, weight='bold')

            ev = prob * payoff

            ax.add_patch(
                patches.FancyBboxPatch(
                    (7.2, opt_y - 0.25),
                    0.8,
                    0.5,
                    boxstyle="round",
                    fc='lightyellow',
                    ec='darkgreen' if is_best else 'black',
                    linewidth=lw
                )
            )
            ax.text(7.6, opt_y + 0.08,
                    f'${payoff}',
                    ha='center',
                    fontsize=8, weight='bold')
            ax.text(7.6, opt_y - 0.12,
                    f'EV=${ev:.0f}',
                    ha='center',
                    fontsize=7, color='green')

    ax.text(
        5, 5.7,
        f' BEST: {best[0]} → {best[1]} (EV=${best[2]:.1f})',
        ha='center',
        fontsize=11,
        weight='bold',
        bbox=dict(boxstyle='round', fc='gold', alpha=0.8)
    )

    plt.title('Decision Tree Analysis',
              fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig('decision_tree.png',
                dpi=300, bbox_inches='tight')

    print(
        f"Best Decision: {best[0]} → {best[1]} (EV=${best[2]:.1f})"
    )
    plt.show()


if __name__ == "__main__":
    visualize_decision_tree(decision_tree)

============================================================================================================================

PRACTICAL 6 - Sampling and Sampling Distributions: 
a. Conduct a survey and collect data from a sample population, ensuring proper 
sampling techniques are employed.

import random 
# Define the population of participants with Indian names
population = [ 
{"id": 1, "name": "Aarav", "age": 25, "gender": "Male", "favorite_food": "Pizza"}, {"id": 2, "name": "Aarushi", "age": 30, "gender": "Female", "favorite_food": "Sushi"}, {"id": 3, "name": "Ishaan", "age": 35, "gender": "Male", "favorite_food": "Burger"}, {"id": 4, "name": "Diya", "age": 28, "gender": "Female", "favorite_food": "Pasta"}, {"id": 5, "name": "Aditya", "age": 40, "gender": "Male", "favorite_food": "Steak"}, {"id": 6, "name": "Kavya", "age": 22, "gender": "Female", "favorite_food": "Salad"}, {"id": 7, "name": "Aryan", "age": 33, "gender": "Male", "favorite_food": "Tacos"}, {"id": 8, "name": "Ananya", "age": 29, "gender": "Female", "favorite_food": "Ramen"}, {"id": 9, "name": "Arjun", "age": 27, "gender": "Male", "favorite_food": "Chicken Wings"}, 
{"id": 10, "name": "Khushi", "age": 31, "gender": "Female", "favorite_food": "Sushi"} ] 
# Define the sample size
sample_size = 5
# Randomly select participants for the sample
sample = random.sample(population, sample_size)
# Display the sample
print("Sampled Participants:")
for participant in sample:
    print(participant)

b. Use the Central Limit Theorem to analyze the sampling distribution of a sample 
mean and estimate population parameters.

import numpy as np
import matplotlib.pyplot as plt
# Parameters for the population distribution 
population_mean = 50 
population_std = 10 
population_size = 10000
# Generate data from the population distribution 
population_data = np.random.normal(population_mean, population_std, population_size) 
# Sample parameters 
sample_size = 100 
num_samples = 1000 
# Calculate sample means 
sample_means = [] 
for _ in range(num_samples): 
    sample = np.random.choice(population_data, sample_size, replace=False) 
    sample_mean = np.mean(sample) 
    sample_means.append(sample_mean)
# Plot the sampling distribution of the sample mean 
plt.figure(figsize=(10, 6)) 
plt.hist(sample_means, bins=30, color='skyblue', edgecolor='black', alpha=0.7) 
plt.title('Sampling Distribution of Sample Mean') 
plt.xlabel('Sample Mean') 
plt.ylabel('Frequency') 
plt.grid(True) 
plt.show() 
# Estimate population parameters using sample statistics 
estimated_population_mean = np.mean(sample_means) 
estimated_population_std = np.std(sample_means) 
print("Estimated Population Mean:", estimated_population_mean) 
print("Estimated Population Standard Deviation:", estimated_population_std)

============================================================================================================================

PRACTICAL 7 - Confidence Interval Estimation: 
a. Calculate confidence intervals for population means or proportions using 
sample data and interpret the results in a business context. 
b. Apply bootstrapping techniques to estimate confidence intervals for non
parametric statistics.

import numpy as np
from scipy.stats import t
# Sample data (example)
sample_data = [32, 45, 28, 36, 39, 41, 38, 49, 50, 44]
# Sample statistics
sample_mean = np.mean(sample_data)
sample_std = np.std(sample_data, ddof=1) # Use ddof=1 for sample standard deviation
sample_size = len(sample_data)
# Confidence level and degrees of freedom
confidence_level = 0.95
degrees_of_freedom = sample_size - 1
# Calculate t-score (for small sample size)
t_score = t.ppf((1 + confidence_level) / 2, degrees_of_freedom)
# Calculate margin of error
margin_of_error = t_score * (sample_std / np.sqrt(sample_size))
# Calculate confidence interval
lower_bound = sample_mean - margin_of_error
upper_bound = sample_mean + margin_of_error
print("Sample Mean:", sample_mean)
print("Margin of Error:", margin_of_error)
print("Confidence Interval:", (lower_bound, upper_bound))
import numpy as np
# Sample data (example)
sample_data = np.array([32, 45, 28, 36, 39, 41, 38, 49, 50, 44])
# Define the number of bootstrap samples
num_bootstraps = 1000
# Function to compute median from bootstrap sample
def compute_median(bootstrap_sample):
    return np.median(bootstrap_sample)
# Generate bootstrap samples and compute medians
bootstrap_medians = []
for _ in range(num_bootstraps):
    bootstrap_sample = np.random.choice(sample_data,

size=len(sample_data), replace=True)
bootstrap_median = compute_median(bootstrap_sample)
bootstrap_medians.append(bootstrap_median)
# Calculate confidence interval (e.g., 95%)
confidence_level = 0.95
lower_percentile = (1 - confidence_level) / 2 * 100
upper_percentile = (1 + confidence_level) / 2 * 100
confidence_interval = np.percentile(bootstrap_medians, [lower_percentile,
upper_percentile])
print("Bootstrap Confidence Interval for Median:", confidence_interval)

===========================================================================================================================

PRACTICAL 8 - Hypothesis Testing: 
a. Formulate null and alternative hypotheses related to a business problem, 
conduct a hypothesis test using appropriate statistical tests, and interpret the 
results. 
b. Perform A/B testing on a website or marketing campaign to evaluate the 
effectiveness of different strategies and make data-driven decisions. 

import numpy as np
from scipy import stats
# Example data for hypothesis testing (sales revenue) 
strategy_A_sales = np.array([1000, 1200, 1100, 1300, 1400]) 
strategy_B_sales = np.array([900, 1100, 1000, 1200, 1300])
# Conduct two-sample t-test for hypothesis testing 
t_statistic, p_value = stats.ttest_ind(strategy_A_sales, strategy_B_sales) 
print("Hypothesis Testing Results:") 
print("t-statistic:", t_statistic) 
print("p-value:", p_value) 

# Example data for A/B testing (conversion rates) 
visitors_A = 1000 
conversions_A = 100 
visitors_B = 1200 
conversions_B = 130 

# Conduct z-test for proportions for A/B testing 
p_A = conversions_A / visitors_A 
p_B = conversions_B / visitors_B 
p_pool = (conversions_A + conversions_B) / (visitors_A + visitors_B) 
z_statistic = (p_A - p_B) / np.sqrt(p_pool * (1 - p_pool) * (1/visitors_A + 1/visitors_B)) 
p_value_AB = 2 * (1 - stats.norm.cdf(np.abs(z_statistic))) 
# Two-tailed test 
print("\nA/B Testing Results:") 
print("z-statistic:", z_statistic) 
print("p-value:", p_value_AB) 


============================================================================================================================


PRACTICAL 9 - Regression Analysis and Time Series Analysis: 
a. Develop a regression model to predict future sales based on historical data, 
assess model performance, and interpret the significance of predictor variables. 
b. Apply time series analysis techniques (e.g., ARIMA, exponential smoothing) 
to forecast future demand for a product or service, and evaluate the accuracy 
of the forecasts. 

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error 
from statsmodels.tsa.arima.model import ARIMA 
from statsmodels.tsa.holtwinters import ExponentialSmoothing 
# Generate sample sales data 
np.random.seed(0) 
dates = pd.date_range(start='2020-01-01', periods=100, freq='D') 
sales = 100 + np.random.randn(100).cumsum() 
sales_data = pd.DataFrame({'Date': dates, 'Sales': sales})
# Part a: Regression Analysis 
def regression_analysis(data): 
    X = pd.to_numeric(data['Date']).values.reshape(-1, 1) 
    y = data['Sales'].values 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
    model = LinearRegression() 
    model.fit(X_train, y_train) 
    y_pred = model.predict(X_test) 
    mse = mean_squared_error(y_test, y_pred)  
    print("Mean Squared Error (MSE) for regression model:", mse) 
    # Plot actual vs predicted sales 
    plt.figure(figsize=(10, 6)) 
    plt.plot(data['Date'], data['Sales'], label='Actual Sales') 
    plt.scatter(pd.to_datetime(X_test.flatten()), y_pred, label='Predicted Sales')
    plt.title('Regression Model: Actual vs Predicted Sales') 
    plt.xlabel('Date') 
    plt.ylabel('Sales') 
    plt.legend() 
    plt.show() 
    # Interpret significance of predictor variables (coefficients) 
    print("Intercept:", model.intercept_) 
    print("Coefficient (slope):", model.coef_[0]) 
# Part b: Time Series Analysis - ARIMA 
def time_series_analysis_arima(data): 
    model = ARIMA(data['Sales'], order=(5,1,0)) # ARIMA(p,d,q) - example order (p=5, d=1, q=0) 
    model_fit = model.fit() 
    # Forecasting 
    forecast = model_fit.forecast(steps=10) # Forecasting 10 future steps print("ARIMA Forecasted Sales:", forecast)
    # Plotting 
    plt.figure(figsize=(10, 6)) 
    plt.plot(data['Date'], data['Sales'], label='Actual Sales') 
    plt.plot(pd.date_range(start=data['Date'].iloc[-1], periods=11, freq='D')[1:], forecast, label='Forecasted Sales') 
    plt.title('ARIMA Forecast: Actual vs Forecasted Sales') 
    plt.xlabel('Date') 
    plt.ylabel('Sales') 
    plt.legend() 
    plt.show() 
# Part b: Time Series Analysis - Exponential Smoothing 
def time_series_analysis_exponential_smoothing(data): 
    model = ExponentialSmoothing(data['Sales'], seasonal_periods=7, trend='add', seasonal='add') 
    model_fit = model.fit() 
    # Forecasting 
    forecast = model_fit.forecast(steps=10) # Forecasting 10 future steps print("Exponential Smoothing Forecasted Sales:", forecast) 
    # Plotting 
    plt.figure(figsize=(10, 6)) 
    plt.plot(data['Date'], data['Sales'], label='Actual Sales') 
    plt.plot(pd.date_range(start=data['Date'].iloc[-1], periods=11, freq='D')[1:], forecast, label='Forecasted Sales') 
    plt.title('Exponential Smoothing Forecast: Actual vs Forecasted Sales') 
    plt.xlabel('Date') 
    plt.ylabel('Sales') 
    plt.legend() 
    plt.show() 
# Main function 
def main(): 
    print("Regression Analysis:") 
    regression_analysis(sales_data) 
    print("\nTime Series Analysis - ARIMA:")
    time_series_analysis_arima(sales_data) 
    print("\nTime Series Analysis - Exponential Smoothing:") 
    time_series_analysis_exponential_smoothing(sales_data) 
if __name__ == "__main__": 
    main() 

===============================================================================================================================

PRACTICAL 10 - Optimization Modeling and Simulation Modeling: 
a. Formulate an optimization model (e.g., linear programming, integer 
programming) to solve a real-world business problem and analyze the optimal 
solution. 
b. Use simulation modeling to evaluate different business scenarios, such as 
capacity planning, inventory management, or pricing strategies, and assess 
their impact on performance metrics. 

!pip install pulp
from pulp import LpMaximize, LpProblem, LpVariable
# Define the problem
model = LpProblem(name="maximize_profit", sense=LpMaximize)
# Define decision variables
x_A = LpVariable(name="x_A", lowBound=0)
x_B = LpVariable(name="x_B", lowBound=0)
# Define objective function
profit_per_A = 10 # Profit per unit of Product A
profit_per_B = 15 # Profit per unit of Product B
model += (profit_per_A * x_A + profit_per_B * x_B)
# Define constraints
resource_constraint = 100 # Total available resource
resource_usage_A = 3 # Resource usage per unit of Product A
resource_usage_B = 4 # Resource usage per unit of Product B
model += (resource_usage_A * x_A + resource_usage_B * x_B <= resource_constraint)
# Solve the problem
model.solve()
# Print optimal solution
print("Optimal Solution:")
print("Product A:", x_A.varValue)
print("Product B:", x_B.varValue)
print("Maximum Profit:", model.objective.value())
import numpy as np
import matplotlib.pyplot as plt
# Parameters
initial_inventory = 100
mean_demand = 50
std_demand = 10
num_periods = 100
# Simulate inventory levels over time
inventory_levels = [initial_inventory]
for _ in range(num_periods):
    demand = np.random.normal(mean_demand, std_demand) # Simulate demand
    inventory_level = max(0, inventory_levels[-1] - demand) # Calculate new inventory level
    inventory_levels.append(inventory_level)
# Plot inventory levels over time
plt.plot(range(num_periods + 1), inventory_levels)
plt.title('Inventory Level over Time')
plt.xlabel('Time Period')
plt.ylabel('Inventory Level')
plt.grid(True)
plt.show()

======================================================================================================================================

PRACTICAL 11 - Analysis of Variance and Experimental Design: 
a. Design and conduct an experiment to study the effects of different factors on a 
specific response variable, analyze the results using analysis of variance 
(ANOVA), and draw conclusions. 
b. Implement a factorial experiment and analyze the main effects and interaction 
effects of factors using statistical techniques. 

import numpy as np
import pandas as pd
from scipy.stats import f_oneway

# Define factors and levels
fertilizer_types = ['Organic', 'Chemical']
watering_freq = ['Daily', 'Alternate-day']
num_replications = 5 # Number of replications for each treatment combination

# Simulate data for plant height
np.random.seed(42) # for reproducibility
data = []
for fert in fertilizer_types:
    for water in watering_freq:
        heights = np.random.normal(loc=30, scale=5, size=num_replications) # Simulate plant heights
        data.extend(zip([fert]*num_replications, [water]*num_replications, heights))

# Create DataFrame
df = pd.DataFrame(data, columns=['Fertilizer', 'Watering', 'Height'])

# Perform ANOVA
anova_results = {}
for factor in ['Fertilizer', 'Watering']:
    factor_levels = df[factor].unique()
    samples = [df[df[factor] == level]['Height'] 
               for level in factor_levels]
    anova_results[factor] = f_oneway(*samples)

# Calculate interaction effect
interaction_effect = df.groupby(['Fertilizer',
'Watering'])['Height'].mean().unstack().diff(axis=1).iloc[:, -1]

# Print ANOVA results
print("ANOVA Results:")
for factor, result in anova_results.items():
    print(f"{factor}: F-value = {result.statistic}, p-value = {result.pvalue}")

# Print interaction effect
print("\nInteraction Effect:")
print(interaction_effect)

